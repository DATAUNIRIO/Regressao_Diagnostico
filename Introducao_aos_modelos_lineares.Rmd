---
title: "Introdução aos Modelos Lineares"
author: "Steven Dutt Ross"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    lib_dir: libs
code_folding: hide
css: xaringan-themer.css
nature:
  highlightStyle: github
highlightLines: true
countIncrementalSlides: false
fig_width: 12 
fig_height: 10 
---
  
```{r setup, include=FALSE, eval=TRUE}
# color: #0d2f47;
options(htmltools.dir.version = FALSE)
library(xaringanthemer)
library(knitr)
solarized_light(header_color="#b5244a")
library(ggplot2)
library(dplyr)
source('https://raw.githubusercontent.com/DATAUNIRIO/Base_de_dados/master/Scripts/11_construcao_da_base_de_dados_carros.R')

```


## Como ajustar uma reta que a esses dados?
```{r, echo=FALSE, eval=TRUE}
qplot(Kmporlitro, HP, data = CARROS)
```

---

class: inverse, center, middle

# Introdução aos Modelos de Regressão



---

## Introdução aos Modelos Lineares 

### Representação matemática de um fenômeno  
### É utilizado para descrever relações entre variáveis  
<br>
### Modelos Determinísticos  
### Modelos Probabilísticos  


.footnote[
[1] Fonte deste segmento: [© 2013 Denise Britz do Nascimento Silva](http://www.ence.ibge.gov.br/index.php/portal-graduacao/portal-graduacao-corpo-docente)
]

---

## Introdução aos Modelos Lineares 

### Modelo Estatístico
Extração de sinal na presença de ruídos  
- a transmissão e recepção de informações pode ser decomposta em sinal e ruído  
<br>
#### Sinal  
- componente determinístico, não aleatório  
<br>
#### Ruído  
- componente aleatório

---

## Modelo Estatístico

Resume as principais características de um conjunto de dados, substituindo seus valores individuais por funções matemáticas. <br>
<br>
Representa a informação através de um número restrito de quantidades/variáveis (parâmetros).

---

## Modelo Estatístico

### A interpretação/resumo das informações de um conjunto de dados demanda a formulação e identificação de padrões que possam representar as informações.

### Busca-se evidenciar suas principais características, utilizando um número restrito de variáveis/quantidades.


---

## Modelo Determinístico
### Representa relações exatas

Exemplo: <br>
– Relação entre força, massa e aceleração <br>
– F = m·a <br>
 <br>
  <br>
– Relação entre velocidade, distância e tempo <br>
– V = d/t  <br>

---

## Modelo Probabilístico

### Considera dois componentes

#### O componente determinístico  
#### O componente aleatório  

Exemplo:  
– O volume de vendas é, em geral, 10 vezes o que se gasta com propaganda...*exatamente?*  

– Y = 10X + Erro  

– O erro aleatório representa a variação nas vendas que não pode ser explicada pela propaganda

---

# Modelos de Regressão
### Qual é a relação entre as diferentes variáveis?  

### Variável reposta: uma variável numérica (aleatória), cujo comportamento (tendência) deseja-se analisar ou prever

### Variáveis explicativas ou independentes: uma ou mais variáveis numéricas ou categóricas não estocásticas que se supõe sejam relacionadas à variável reposta.

$$Y_i = \beta_0+\beta_1*X_i+\epsilon$$

---

```{r, echo=FALSE, eval=TRUE}
fit <- lm(Kmporlitro ~ HP, data = CARROS)  # Fit the model
CARROS$predicted <- predict(fit)   # Save the predicted values
CARROS$residuals <- residuals(fit) # Save the residual values
ggplot(CARROS, aes(x = HP, y = Kmporlitro)) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Plot regression slope
  geom_segment(aes(xend = HP, yend = predicted), alpha = .2) +  # alpha to fade lines
  geom_point() +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()  # Add theme for cleaner look

```


---


```{r, echo=FALSE, eval=TRUE}

ggplot(CARROS, aes(x = HP, y = Kmporlitro)) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Plot regression slope
  geom_line(aes(x = HP, y = 20))+
  geom_segment(aes(xend = HP, yend = predicted), alpha = .2) +  # alpha to fade lines
  geom_point() +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()  # Add theme for cleaner look


```

---

# Como ajustar essa reta aos pontos?
```{r ,echo=FALSE, eval=TRUE}
set.seed(15062016)
#X<-rnorm(100, sd=10)
X<-rnorm(100, sd=100)
Y <- -X + rnorm(100, sd=13)
# Diagrama de dispersão
plot(X, Y,pch=19)

```

---

```{r ,echo=FALSE, eval=TRUE}
set.seed(15062016)
X<-rnorm(100, sd=100)
Y <- -X + rnorm(100, sd=13)
# Diagrama de dispersão
plot(X, Y,pch=19)
abline(a = 1, b = -4, col = "skyblue")


```

---

```{r ,echo=FALSE, eval=TRUE}
set.seed(15062016)
X<-rnorm(100, sd=100)
Y <- -X + rnorm(100, sd=13)
# Diagrama de dispersão
plot(X, Y,pch=19)
abline(a = 1, b = -4, col = "skyblue")
abline(a = 1, b = -2, col = "blue")
```

---

```{r ,echo=FALSE, eval=TRUE}
set.seed(15062016)
X<-rnorm(100, sd=100)
Y <- -X + rnorm(100, sd=13)
# Diagrama de dispersão
plot(X, Y,pch=19)
abline(a = 1, b = -4, col = "skyblue")
abline(a = 1, b = -2, col = "blue")
abline(a = 1, b = 0, col = "green")
```

---

```{r ,echo=FALSE, eval=TRUE}
set.seed(15062016)
X<-rnorm(100, sd=100)
Y <- -X + rnorm(100, sd=13)
# Diagrama de dispersão
plot(X, Y,pch=19)
abline(a = 1, b = -4, col = "skyblue")
abline(a = 1, b = -2, col = "blue")
abline(a = 1, b = 0, col = "green")
z <- lm(Y ~ X)
abline(z, col = "red")
```

---

class: center, middle, inverse


# QUAL A MELHOR RETA?

---

# Como podemos ajustar a reta (distância minima ao quadrado)?

```{r ,echo=FALSE, eval=TRUE}
y<-c(10,12,13,15,16,18,19,20,21,22)  
x<-c(1,2,3,4,5,6,7,8,9,10)
plot(x,y,pch = 19, col = 'darkgray', las = 1)
abline(lm(y~x), lwd = 2, col="red")
```

---

## Método dos Mínimos Quadrados
```{r}
y<-c(10,12,13,15,16,18,19,20,21,22)  
x<-c(1,2,3,4,5,6,7,8,9,10)
mod<-lm(y~x)
mod
Yhat <- predict(mod)
Erro <- c(y - Yhat) # equivalente a residuals(mod)

```

---

Para esse conjunto de dados, as estimativas do $\beta_0$ e do $\beta_1$ desse modelo linear simples são `r round(coef(mod)[1],2)` e `r round(coef(mod)[2],2)`. Logo a equação fica:

$\hat{Y}$ = `r round(coef(mod)[1],2)` + `r round(coef(mod)[2],2)`*Xi

Por exemplo, para dado valor de X, é possível obter um valor para Y  <br>
X = 1 ->  $\hat{Y}$ = 9,27+1,33*(1) = 10,6 (como y = 10 o erro é -0,6)  <br>
<br>
X = 5 ->  $\hat{Y}$ = 9,27+1,33*(5) = 15,932 (como y = 16 o erro é 0,068)  

---

## Tipos de relaciomentos

```{r, eval=FALSE}
y<-c(10,11,12,13,14,15,16,17,18,19)
x<-c(1,2,3,4,5,6,7,8,9,10)
y2<-c(10,12,13,15,16,18,19,20,21,22)
x2<-c(1,3,5,6,8,9,6,13,14,20)
y3<-c(9,12,13,15,16,18,19,19,18,15)
x3<-c(1,3,5,6,8,9,6,13,14,20)
y4<-c(10,12,13,10,12,13,14,10,11,12)
x4<-c(1,3,5,6,8,9,6,13,14,20)

par(mfrow=c(2,2))
plot(x,y)
abline(lm(y~x),col="red")
plot(x2,y2)
abline(lm(y2~x2),col="blue")
plot(x3,y3)
abline(lm(y3~x3),col="darkgreen")
plot(x4,y4)
abline(lm(y4~x4),col="gray")
```

---

```{r, echo=FALSE, fig.height=9, fig.width=10}
y<-c(10,11,12,13,14,15,16,17,18,19)
x<-c(1,2,3,4,5,6,7,8,9,10)
y2<-c(10,12,13,15,16,18,19,20,21,22)
x2<-c(1,3,5,6,8,9,6,13,14,20)
y3<-c(9,12,13,15,16,18,19,19,18,15)
x3<-c(1,3,5,6,8,9,6,13,14,20)
y4<-c(10,12,13,10,12,13,14,10,11,12)
x4<-c(1,3,5,6,8,9,6,13,14,20)

par(mfrow=c(2,2))
plot(x,y)
abline(lm(y~x),col="red")
plot(x2,y2)
abline(lm(y2~x2),col="blue")
plot(x3,y3)
abline(lm(y3~x3),col="darkgreen")
plot(x4,y4)
abline(lm(y4~x4),col="gray")
```


---

## Métodos dos Mínimos Quadrados

```{r}
anova(mod)
```


---

## Métodos dos Mínimos Quadrados

```{r}
summary(mod)
```

---


```{r}
fit <- lm(Kmporlitro ~ HP, data = CARROS)  # Fit the model
summary(fit)  # Report the results
```

---

```{r}
CARROS$predicted <- predict(fit)   # Save the predicted values
CARROS$residuals <- residuals(fit) # Save the residual values
CARROS %>% select(Kmporlitro, predicted, residuals) %>% head()

```

---

```{r}
confint(fit, level=0.95) # intervalo de confianca dos parametros do modelo
fitted(fit) # valores ajustados
```

---
class: center, middle

# Visualizando o Modelo

---

```{r}

ggplot(CARROS, aes(x = HP, y = Kmporlitro)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Plot regression slope
  theme_bw()  # Add theme for cleaner look


```

---

```{r}

ggplot(CARROS, aes(x = HP, y = Kmporlitro)) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Plot regression slope
  geom_segment(aes(xend = HP, yend = predicted), alpha = .2) +  # alpha to fade lines
  geom_point() +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()  # Add theme for cleaner look


```

---

```{r}
ggplot(CARROS, aes(x = HP, y = Kmporlitro)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(xend = HP, yend = predicted), alpha = .2) +
  geom_point(aes(color = abs(residuals), size = abs(residuals))) + # size also mapped
  scale_color_continuous(low = "black", high = "red") +
  guides(color = FALSE, size = FALSE) +  # Size legend also removed
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()

```

---
class: inverse, center, middle

# Regressão Múltipla

---

# Regressão Múltipla

Queremos ajustar o modelo de minimos quadrados (MQO) com mais de uma variável explicativa. 

Por exemplo:

$$KmporLitro = \beta_0+\beta_1*HP+\beta_2*Peso+\epsilon$$

$$KmporLitro = \beta_0+\beta_1*HP+\beta_2*Peso+\beta_3*TipodeMarcha +\beta_4 Tipodecombustivel+\epsilon$$
 
O modelo 1 tem duas variáveis explicativas.  
O modelo 2 tem quatro variáveis explicativas.

Três perguntas:   
1) como fazer esses modelos?  
2) qual deles tem o melhor ajuste?    
3) como interpretar o coeficiente beta?  

---

```{r, eval=FALSE}
### Regressao linear Multipla
modelo1 = lm(Kmporlitro ~ HP + Peso , data=CARROS)
modelo2 = lm(Kmporlitro ~ HP + Peso + TipodeMarcha, data=CARROS)
modelo_geral = lm(Kmporlitro ~ HP + Peso + TipodeMarcha + Tipodecombustivel, data=CARROS)

summary(modelo1)
summary(modelo2)
summary(modelo_geral)
```

---

```{r}
### Regressao linear Multipla
modelo1 = lm(Kmporlitro ~ HP + Peso , data=CARROS)
modelo2 = lm(Kmporlitro ~ HP + Peso + TipodeMarcha + Tipodecombustivel, data=CARROS)
modelo_geral = lm(Kmporlitro ~ HP + Peso + TipodeMarcha + Tipodecombustivel + NumdeValvulas + NumdeMarchas + RPM + Cilindros, data=CARROS)

summary(modelo1)
#summary(modelo2)
#summary(modelo_geral)
```

---

### A previsão
Para fins de teste, vamos tentar prever o MPG para o carro Valiant:

```{r}
data("mtcars")
modelo_geral = lm(mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb , data=mtcars)

df <- data.frame(cyl = 6, disp = 225, hp = 105, drat = 2.76, wt = 3.460, qsec = 20.22, vs = 1, am = 0, gear = 3, carb = 1)
predict(modelo_geral, df)

```
O MPG real para este carro é 18,1.

---

# R2 e R2 ajustado

```{r}
summary(modelo1)$r.squared 
summary(modelo2)$r.squared 
summary(modelo_geral)$r.squared 

summary(modelo1)$adj.r.squared 
summary(modelo2)$adj.r.squared 
summary(modelo_geral)$adj.r.squared 
```

---

# R2 e Akaike information criterion (AIC)
```{r}
summary(modelo1)$adj.r.squared 
summary(modelo2)$adj.r.squared 
summary(modelo_geral)$adj.r.squared 
AIC(modelo1)
AIC(modelo2)
AIC(modelo_geral)
```

---

### Seleção de modelo (variáveis) via STEPWISE
```{r, message=FALSE, warning=FALSE}
library(MASS)
step <- stepAIC(modelo_geral, direction="both") # quanto menor o AIC mais parcimonioso o modelo
step$anova # display results 
```

---

# ANOVA
```{r, message=FALSE, warning=FALSE, eval=FALSE}
m1 <- aov(CARROS$Kmporlitro ~ as.factor(CARROS$NumdeValvulas))
summary(m1)
layout(matrix(c(1,2,3,4),2,2)) # optional layout
plot(m1) # diagnostic plots
TukeyHSD(m1)
plot(TukeyHSD(m1)) # where fit comes from aov() 
```


---

# ANOVA
```{r, message=FALSE, warning=FALSE, eval=FALSE}
m1 <- aov(CARROS$Kmporlitro ~ as.factor(CARROS$Cilindros) + CARROS$TipodeMarcha)
summary(m1)
layout(matrix(c(1,2,3,4),2,2)) # optional layout
plot(m1) # diagnostic plots
TukeyHSD(m1)
plot(TukeyHSD(m1)) # where fit comes from aov() 
```


---


<!--
background-image: url(http://www.fondazioneluigieinaudi.it/wp-content/uploads/2018/01/pol.jpg)
background-position: 50% 50%
class: center, top, inverse
   # Use R!
---
class: center, middle, inverse
class: center, bottom, inverse

-->